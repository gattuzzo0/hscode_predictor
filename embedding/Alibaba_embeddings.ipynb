{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "#from langchain_openai  import OpenAIEmbeddings\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "#from InstructorEmbedding import INSTRUCTOR\n",
    "#from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import warnings, re\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up embedding model to use with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model definition\n",
    "A technique for representing text data as numerical vectors, which can be input into machine learning models. The embedding model is responsible for converting text into these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModel\n",
    "\n",
    "#Check HuggingFace leaderboard: https://huggingface.co/spaces/mteb/leaderboard\n",
    "#dunzhang/stella_en_1.5B_v5\n",
    "#Alibaba-NLP/gte-large-en-v1.5\n",
    "#jinaai/jina-embeddings-v2-base-en\n",
    "model = AutoModel.from_pretrained('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True) \n",
    "\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cuda:0', 'trust_remote_code':True}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_via_Transformers_class = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO CHUNK Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Chapter 10   Cereals' metadata={'source': '10'}\n",
      "page_content='Wheat and meslin.' metadata={'source': '10.01'}\n",
      "Amount of documents is:  23514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando embeddings...: 100%|██████████| 23514/23514 [14:31<00:00, 26.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir =  os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir,'..' ,'index', 'Alibaba-extended'))\n",
    "\n",
    "# Importante Leer columna de codigo como string, sino se eliminan los zeros a la izquierda.\n",
    "df = pd.read_csv('..\\data\\WebScrap_CSVs\\hts_codes_WebScrapped.csv', encoding='utf-8', dtype={'HTS code': str})\n",
    "\n",
    "# Split data and metadata\n",
    "texts = df['Description'].tolist()  # This is the text data that will be embedded\n",
    "metadata = df['HTS code'].tolist()  # This is the metadata that will be stored alongside the embeddings\n",
    "\n",
    "documents = []\n",
    "for i, text in enumerate(texts):\n",
    "    document = Document(page_content=text, metadata={\"source\": metadata[i]})\n",
    "    documents.append(document)\n",
    "\n",
    "# Debug to check metadata + text\n",
    "print(documents[0])\n",
    "print(documents[1])    \n",
    "print(\"Amount of documents is: \" , len(documents))   \n",
    "\n",
    "vector_db = None\n",
    "with tqdm(total=len(documents), desc=\"Creando embeddings...\") as pbar:\n",
    "    for d in documents:\n",
    "        if vector_db:\n",
    "            vector_db.add_documents([d])\n",
    "        else:\n",
    "            #When no GPU is available\n",
    "            #vector_db = Chroma.from_documents([d],embed_model, persist_directory=persistent_dir )\n",
    "            \n",
    "            #To enable embeddings running on GPU: embedding and ingesting at the same time\n",
    "            vector_db = Chroma.from_documents([d],embedding_model_via_Transformers_class, persist_directory=persistent_dir)\n",
    "        pbar.update(1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHORT SIZE chunk strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando embeddings...: 100%|██████████| 10401/10401 [06:08<00:00, 28.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir = os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir, '..', 'index', 'Alibaba-chunk50-overlap0'))\n",
    "\n",
    "# Step 1: Read the .txt file\n",
    "file_path = '..\\data\\hs_code_dictionary.txt'  # Path to your .txt file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Function to extract metadata (HS code) and content\n",
    "def extract_metadata_and_content(line):\n",
    "    match = re.match(r\"(\\d{4}\\.\\d{2})\\s*(.*)\", line)\n",
    "    if match:\n",
    "        metadata = match.group(1)  # The number part as metadata\n",
    "        content = match.group(2)   # The rest of the line as content\n",
    "        return metadata, content\n",
    "    return None, line  # In case no match, return the line as-is\n",
    "\n",
    "# Step 2: Split the text into lines first\n",
    "lines = text.split(\"\\n\")\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "chunk_size = 50  # Define your chunk size\n",
    "chunk_overlap = 0  # Set to 0 if you don't want overlapping chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Step 3: Process each line, extract metadata, split content, and create Document objects\n",
    "documents = []\n",
    "for line in lines:\n",
    "    metadata, content = extract_metadata_and_content(line)\n",
    "    \n",
    "    if metadata:  # If we found valid metadata\n",
    "        # Split the content into chunks\n",
    "        chunks = text_splitter.split_text(content)\n",
    "        \n",
    "        # Create a Document object for each chunk, with metadata containing the HS code\n",
    "        for chunk in chunks:\n",
    "            document = Document(page_content=chunk, metadata={\"hs_code\": metadata, \"source\": file_path})\n",
    "            documents.append(document)\n",
    "\n",
    "# Step 4: Embed the documents into the vector database\n",
    "vector_db = None\n",
    "with tqdm(total=len(documents), desc=\"Creando embeddings...\") as pbar:\n",
    "    for d in documents:\n",
    "        if vector_db:\n",
    "            vector_db.add_documents([d])\n",
    "        else:\n",
    "            # When no GPU is available, initialize vector_db\n",
    "            #vector_db = Chroma.from_documents([d], embed_model, persist_directory=persistent_dir)\n",
    "            \n",
    "            # To enable embeddings running on GPU, ingest documents and create embeddings\n",
    "            vector_db = Chroma.from_documents([d], embedding_model_via_Transformers_class, persist_directory=persistent_dir)\n",
    "        pbar.update(1)\n",
    "\n",
    "# The 'documents' list now contains Document objects with metadata and chunks, indexed into the vector database\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
